Utilitarul Web Crawler descarcă recursiv paginile din Internet, găsește rezultatele descărcării după nume/ tip/ dimensiune și creează un sitemap aferent.

OPȚIUNI       

	-c, --crawl	<config.conf> <SITES_FILE>			
						funcția de bază; descarcă recursiv paginile specificate în 
						SITES_FILE conform opțiunilor din CONFIG_FILE; vezi mai jos
						formatul celor 2 fișiere	
	-s, --sitemap <ROOT_DIR> <SITE_NAME>	creează un fișier "sitemap.txt" pentru site-ul specificat,
						deja descărcat în folderul părinte ROOT_DIR; SITE_NAME este
						numele folderului al carui sitemap se doreste a se obtine 
	-e, --search <FOLDER_TO_SEARCH> <KEYWORD>
						caută recursiv în FOLDER_TO_SEARCH și afișează toate 
						path-urile către fișierele al căror nume conține KEYWORD
	-l, --list <FOLDER_TO_SEARCH> <EXTENSION>
						caută recursiv în FOLDER_TO_SEARCH și afișează toate 
						path-urile către fișierele cu extensia EXTENSION
	--help, -?			afișează acest meniu de ajutor
	
	
SITES_FILE are formatul:
		https://numesite1.ro/index.html
		https://numesite2.ro
		https://wiki.numesite2.ro/
		https://stackoverflow.com/company/about-us
		
CONFIG_FILE trebuie să conțină toate câmpurile de mai jos, fiecare pe o linie nouă și are formatul:
		n_threads=50
		delay=100ms
		root_dir=D:/Download
		log_level=3
		depth_level=3
		max_size=500kB
		files_type=all/png/jpg/js/php


În fișierul "log.txt" sunt jurnalizate evenimentele de eroare, atenționare și informare. 
